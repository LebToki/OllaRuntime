# üöÄ OllaCompiler

**Persistent Execution Layer for Ollama**

OllaCompiler bridges the gap between Ollama's powerful LLM inference and persistent code execution. It provides a stateful local execution environment that allows Ollama-generated code to maintain variables, functions, and context across multiple independent prompts.

![OllaCompiler Dashboard Preview](https://raw.githubusercontent.com/LebToki/OllaCompiler/main/preview.png) *(Placeholder for your preview image)*

## üß† The Problem

Ollama is a powerful inference engine, but it is natively stateless. Each request to an LLM like Llama 3 or DeepSeek is an isolated event. If you ask it to "Define a variable `x = 10`" and then "Multiply `x` by 2," the model loses the context of `x` between calls unless you manually manage complex history and shell environments.

## ‚ú® The Solution: OllaCompiler

OllaCompiler provides a persistent Python `InteractiveConsole` wrapped in a FastAPI backend, creating a seamless bridge between LLM-generated code and persistent execution.

### Key Features

- **Persistent State**: Variables, imported modules, and defined functions stay in memory across sessions
- **FastAPI Bridge**: Easy integration for any application wanting to add execution capabilities to Ollama
- **Premium UI**: A high-contrast, glassmorphic dashboard for real-time monitoring of execution state and memory
- **Docker-Less**: Lightweight and easy to run locally on Windows/Linux/Mac without container overhead
- **Multi-Language Support**: Python execution with extensible architecture for other languages

## üõ†Ô∏è Use Cases

1. **Iterative Data Science**: Ask Ollama to load a dataset, then perform multiple follow-up analysis steps without re-loading the data
2. **Autonomous Coding Agents**: Build agents that can verify their own code by running it and seeing output/errors persistently
3. **Persistent Tooling**: Create custom REPL-based tools where the LLM can "learn" and store utility functions over a long conversation
4. **Educational Sandboxes**: Provide a safe, visual way for users to see how LLMs interact with real code execution
5. **Rapid Prototyping**: Test code snippets generated by LLMs in real-time with immediate feedback

## üöÄ Getting Started

### Prerequisites
- Python 3.10+
- [Ollama](https://ollama.com/) installed and running
- Node.js (for development, optional)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/LebToki/OllaCompiler.git
   cd OllaCompiler
   ```

2. **Install dependencies:**
   ```bash
   pip install fastapi uvicorn python-multipart
   ```

3. **Start Ollama:**
   ```bash
   ollama serve
   ```

### Running OllaCompiler

1. **Start the backend:**
   ```bash
   python main.py
   ```

2. **Open your browser to:**
   ```
   http://localhost:8000
   ```

3. **Start coding with persistent execution!**

## üèóÔ∏è Technical Architecture

### Backend Stack
- **FastAPI**: Modern, fast web framework for building APIs
- **Python InteractiveConsole**: Persistent execution environment
- **CORS Middleware**: Cross-origin resource sharing for frontend integration
- **Static File Serving**: Dashboard and assets delivery

### Frontend Stack
- **Vanilla JavaScript**: Lightweight, no framework dependency
- **Glassmorphic Design**: Modern, high-contrast UI
- **Responsive Layout**: Works on desktop and tablet devices
- **Real-time Updates**: Live variable monitoring and execution feedback

### Execution Flow
1. User inputs code or prompt in the terminal
2. Code parser extracts Python code blocks from markdown
3. FastAPI endpoint receives the code
4. Python runtime executes code in persistent console
5. Results and variables are returned to frontend
6. Dashboard updates in real-time with execution state

## üì¶ API Documentation

### `/api/execute` Endpoint
**Method**: POST  
**Content-Type**: application/json  
**Request Body**:
```json
{
  "prompt": "Your code or natural language prompt"
}
```

**Response**:
```json
{
  "output": "Execution output or error messages",
  "variables": {
    "variable_name": "variable_value"
  }
}
```

### Code Parsing
The system automatically extracts code from markdown-formatted prompts:
```markdown
```python
x = 10
print(x * 2)
```

```python
def greet(name):
    return f"Hello, {name}!"
```

```

### Security Considerations
- **Sandboxed Execution**: Code runs in isolated Python console
- **No Network Access**: Restricted imports and network operations
- **Memory Limits**: Configurable execution limits
- **Error Handling**: Comprehensive error catching and reporting

## üß™ Testing

The project includes a comprehensive test suite:

```bash
python -m unittest test_runtime.py
```

Test coverage includes:
- Basic execution and persistence
- Variable serialization
- Security restrictions
- Error handling
- Syntax and runtime error detection

## ü§ù Contributing

Contributions are welcome! Whether it's adding support for Node.js runtimes, improving the UI, or adding more robust sandboxing, feel free to open a PR.

### Development Setup

1. **Install development dependencies:**
   ```bash
   pip install -r requirements-dev.txt
   ```

2. **Run tests:**
   ```bash
   python -m pytest
   ```

3. **Code formatting:**
   ```bash
   black . && isort .
   ```

### Project Structure
```
OllaCompiler/
‚îú‚îÄ‚îÄ main.py              # FastAPI application entry point
‚îú‚îÄ‚îÄ runtime.py           # Persistent Python execution environment
‚îú‚îÄ‚îÄ parser.py            # Code extraction from markdown
‚îú‚îÄ‚îÄ repl_v2.py           # Alternative REPL implementation
‚îú‚îÄ‚îÄ test_runtime.py      # Comprehensive test suite
‚îú‚îÄ‚îÄ script.js            # Frontend JavaScript logic
‚îú‚îÄ‚îÄ style.css            # Glassmorphic styling
‚îú‚îÄ‚îÄ index.html           # Dashboard interface
‚îî‚îÄ‚îÄ README.md            # This file
```

## üîß Configuration

### Environment Variables
- `PORT`: Server port (default: 8000)
- `CORS_ORIGINS`: Comma-separated list of allowed origins
- `MAX_EXECUTION_TIME`: Maximum execution time in seconds

### Custom Extensions
The architecture supports adding new language runtimes:
```python
from runtime import BaseRuntime

class NodeRuntime(BaseRuntime):
    def execute(self, code: str) -> str:
        # Node.js execution logic
        pass
```

## üõ°Ô∏è Security

### Built-in Protections
- Restricted imports (os, sys, subprocess, etc.)
- No file system access
- No network operations
- Memory usage limits
- Execution time limits

### Recommended Practices
- Run in isolated environment
- Monitor resource usage
- Implement rate limiting
- Regular security audits

## üìã License

This project is open source and available under the MIT License.

## ‚ù§Ô∏è Acknowledgments

- [Ollama](https://ollama.com/) for the amazing LLM platform
- FastAPI team for the excellent web framework
- Community contributors for feedback and improvements

---

Built with ‚ù§Ô∏è for the developer community.  
**"CASHFLOW OVER CLOUT" - 2T Interactive.**